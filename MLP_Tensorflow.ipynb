{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_Tensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQ8Qys6Tx4Nl4DHb5pUJG/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmh100298/MLP_from_Scratch/blob/main/MLP_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "MIyVWq5RVfnX",
        "outputId": "92a88da8-93b0-46f3-de28-3b03c86a8fba"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def load_dataset(flatten = False):\n",
        "  (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "  # normalize x\n",
        "  X_train = X_train.astype(float) / 255.\n",
        "  X_test = X_test.astype(float) / 255.\n",
        "\n",
        "  # we reserve the last 10000 training examples for validation\n",
        "  X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
        "  y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
        "\n",
        "  if flatten:\n",
        "      X_train = X_train.reshape([X_train.shape[0], -1])\n",
        "      X_val = X_val.reshape([X_val.shape[0], -1])\n",
        "      X_test = X_test.reshape([X_test.shape[0], -1])\n",
        "\n",
        "  return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
        "## Printing dimensions\n",
        "print(X_train.shape, y_train.shape)\n",
        "## Visualizing the first digit\n",
        "plt.axis('off')\n",
        "plt.imshow(X_train[0], cmap=\"Greys\");\n",
        "\n",
        "## Changing dimension of input images from N*28*28 to  N*784\n",
        "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
        "X_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\n",
        "\n",
        "print('Train dimension:');print(X_train.shape)\n",
        "print('Test dimension:');print(X_test.shape)\n",
        "\n",
        "##  # chuyen doi label sang one-hot encoded (tien xu ly giup ml nhanh hon)\n",
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.transform(y_test)\n",
        "print('Train labels dimension:');print(y_train.shape)\n",
        "print('Test labels dimension:');print(y_test.shape)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 28, 28) (50000,)\n",
            "Train dimension:\n",
            "(50000, 784)\n",
            "Test dimension:\n",
            "(10000, 784)\n",
            "Train labels dimension:\n",
            "(50000, 10)\n",
            "Test labels dimension:\n",
            "(10000, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iboli76Hg6L-"
      },
      "source": [
        "Now we have processed the data, let's start building our multi-layer perceptron using tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Me9c6rVgzrC",
        "outputId": "3ce3feac-f16a-4d3f-9849-e6a07cc67409"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "s = tf.compat.v1.InteractiveSession() # tf.InteractiveSession() is a way to run tensorflow model directly without instantiating a graph whenever we want to run a model.\n",
        "\n",
        "# khoi tao cac gia tri 784(Input)-512(Hidden layer 1)-256(Hidden layer 2)-10(Output) neural net model\n",
        "num_classes = y_train.shape[1]\n",
        "num_features = X_train.shape[1]\n",
        "num_output = y_train.shape[1]\n",
        "num_layers_0 = 512\n",
        "num_layers_1 = 256\n",
        "starter_learning_rate = 0.001\n",
        "regularizer_rate = 0.1\n",
        "#In tensorflow, we define a placeholder for our input variables and output variables and any variables we want to keep track of.\n",
        "# Placeholders for the input data\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "input_X = tf.compat.v1.placeholder('float32',shape =(None,num_features),name=\"input_X\") #Tensor(\"input_X_1:0\", shape=(None, 784), dtype=float32)\n",
        "input_y = tf.compat.v1.placeholder('float32',shape = (None,num_classes),name='input_Y')\n",
        "## for dropout layer\n",
        "keep_prob = tf.compat.v1.placeholder(tf.float32)\n",
        "\n",
        "## Weights initialized by random normal function with std_dev = 1/sqrt(number of input features)\n",
        "weights_0 = tf.Variable(tf.random.normal([num_features,num_layers_0], stddev=(1/tf.sqrt(float(num_features)))))\n",
        "bias_0 = tf.Variable(tf.random.normal([num_layers_0]))\n",
        "\n",
        "weights_1 = tf.Variable(tf.random.normal([num_layers_0,num_layers_1], stddev=(1/tf.sqrt(float(num_layers_0)))))\n",
        "bias_1 = tf.Variable(tf.random.normal([num_layers_1]))\n",
        "\n",
        "weights_2 = tf.Variable(tf.random.normal([num_layers_1,num_output], stddev=(1/tf.sqrt(float(num_layers_1)))))\n",
        "bias_2 = tf.Variable(tf.random.normal([num_output]))\n",
        "\n",
        "hidden_output_0 = tf.nn.relu(tf.matmul(input_X,weights_0)+bias_0)  # use ReLU activation for hidden layers\n",
        "hidden_output_0_0 = tf.nn.dropout(hidden_output_0, keep_prob)# Dropout is an essential concept in creating redundancies in our network which leads to better generalization.\n",
        "\n",
        "hidden_output_1 = tf.nn.relu(tf.matmul(hidden_output_0_0,weights_1)+bias_1)\n",
        "hidden_output_1_1 = tf.nn.dropout(hidden_output_1, keep_prob)\n",
        "\n",
        "predicted_y = tf.sigmoid(tf.matmul(hidden_output_1_1,weights_2) + bias_2)#  softmax for the final output layer to get class probability score"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1761: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StH-K3yltey7"
      },
      "source": [
        "## Defining the loss function\n",
        "loss = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(logits=predicted_y,labels=input_y))  + regularizer_rate*(tf.reduce_sum(tf.square(bias_0)) + tf.reduce_sum(tf.square(bias_1)))"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbEBeEm8w_I2"
      },
      "source": [
        "# xac dinh  learning rate cho mang de toi uu hoa weight and bias \n",
        "## Variable learning rate\n",
        "learning_rate = tf.compat.v1.train.exponential_decay(starter_learning_rate, 0, 5, 0.85, staircase=True)# use exponential decay on our learning rate by every five epoch to reduce the learning by 15%(0.85)\n",
        "## Adam optimzer for finding the right weight\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss,var_list=[weights_0,weights_1,weights_2,bias_0,bias_1,bias_2])\n",
        "## Metrics definition # define accuracy metric to evaluate our model performance as loss function is non-intuitive.\n",
        "correct_prediction = tf.equal(tf.argmax(y_train,1), tf.argmax(predicted_y,1)) # equal tra ve 1 tensor = shape cua 1 trong 2 (neu ca 2 bang nhau true, true) sai (true,false)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # chuyen sang tensor sang float lay trung binh"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1NM2yqF1Uoy",
        "outputId": "479c4def-250b-4f23-a4b1-a2d6227f93b3"
      },
      "source": [
        "## Training parameters\n",
        "batch_size = 128\n",
        "epochs=14\n",
        "dropout_prob = 0.6\n",
        "\n",
        "training_accuracy = []\n",
        "training_loss = []\n",
        "testing_accuracy = []\n",
        "\n",
        "s.run(tf.compat.v1.global_variables_initializer())\n",
        "for epoch in range(epochs):    \n",
        "    arr = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(arr)\n",
        "    for index in range(0,X_train.shape[0],batch_size):\n",
        "        s.run(optimizer, {input_X: X_train[arr[index:index+batch_size]],\n",
        "                          input_y: y_train[arr[index:index+batch_size]],\n",
        "                        keep_prob:dropout_prob})\n",
        "    training_accuracy.append(s.run(accuracy, feed_dict= {input_X:X_train, \n",
        "                                                         input_y: y_train,keep_prob:1}))\n",
        "    training_loss.append(s.run(loss, {input_X: X_train, \n",
        "                                      input_y: y_train,keep_prob:1}))\n",
        "    \n",
        "    ## Evaluation of model\n",
        "    testing_accuracy.append(accuracy_score(y_test.argmax(1), \n",
        "                            s.run(predicted_y, {input_X: X_test,keep_prob:1}).argmax(1)))\n",
        "    print(\"Epoch:{0}, Train loss: {1:.2f} Train acc: {2:.3f}, Test acc:{3:.3f}\".format(epoch,\n",
        "                                                                    training_loss[epoch],\n",
        "                                                                    training_accuracy[epoch],\n",
        "                                                                   testing_accuracy[epoch]))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:0, Train loss: nan Train acc: 0.099, Test acc:0.098\n",
            "Epoch:1, Train loss: nan Train acc: 0.099, Test acc:0.098\n",
            "Epoch:2, Train loss: nan Train acc: 0.099, Test acc:0.098\n",
            "Epoch:3, Train loss: nan Train acc: 0.099, Test acc:0.098\n",
            "Epoch:4, Train loss: nan Train acc: 0.099, Test acc:0.098\n",
            "Epoch:5, Train loss: nan Train acc: 0.099, Test acc:0.098\n",
            "Epoch:6, Train loss: nan Train acc: 0.099, Test acc:0.098\n",
            "Epoch:7, Train loss: nan Train acc: 0.099, Test acc:0.098\n",
            "Epoch:8, Train loss: nan Train acc: 0.099, Test acc:0.098\n",
            "Epoch:9, Train loss: nan Train acc: 0.099, Test acc:0.098\n",
            "Epoch:10, Train loss: nan Train acc: 0.099, Test acc:0.098\n",
            "Epoch:11, Train loss: nan Train acc: 0.099, Test acc:0.098\n",
            "Epoch:12, Train loss: nan Train acc: 0.099, Test acc:0.098\n",
            "Epoch:13, Train loss: nan Train acc: 0.099, Test acc:0.098\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}